{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcbfc36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 20 steps\n",
      "Episode 2 finished after 28 steps\n",
      "Episode 3 finished after 14 steps\n",
      "Episode 4 finished after 11 steps\n",
      "Episode 5 finished after 49 steps\n",
      "Episode 6 finished after 11 steps\n",
      "Episode 7 finished after 9 steps\n",
      "Episode 8 finished after 35 steps\n",
      "Episode 9 finished after 40 steps\n",
      "Episode 10 finished after 44 steps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# 创建CartPole环境\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "# 进行若干次实验\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    step_count = 0\n",
    "\n",
    "    # 在每次实验中，执行若干步操作\n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        step_count += 1\n",
    "\n",
    "    print(f\"Episode {episode + 1} finished after {step_count} steps\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ac7f0",
   "metadata": {},
   "source": [
    "## 从知乎上找到的一个爬山算法案例，这个算法的特点是：\n",
    "1. 它在1000次的随机过程（一个episode）中只要找到奖赏总值超过200的参数就结束了，门槛较低，因此在随后测试过程（跑一次episode）中平均得分只有100多；\n",
    "2. 评价标准低，算法只用了一个episode的分数，容易出现性能不稳定，但是算法收敛性好；\n",
    "3. 程序不够清晰简洁。\n",
    "\n",
    "[OpenAI Gym 经典控制环境介绍——CartPole（倒立摆）](https://zhuanlan.zhihu.com/p/570695189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07473a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 226.0 [ 0.7842078  -0.13505991  0.76737898  1.02372538 -0.04597676]\n",
      "(226.0, array([ 0.7842078 , -0.13505991,  0.76737898,  1.02372538, -0.04597676]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "def get_action(weights, observation):# 根据权值对当前状态做出决策\n",
    "    wxb = np.dot(weights[:4], observation) + weights[4] # 计算加权和\n",
    "    if wxb >= 0:# 加权和大于0时选取动作1，否则选取0\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_sum_reward_by_weights(env, weights):\n",
    "# 测试不同权值的控制模型有效控制的持续时间（或奖励）\n",
    "    observation = env.reset()[0] # 重置初始状态\n",
    "    sum_reward = 0 # 记录总的奖励\n",
    "    for t in range(1000):\n",
    "        # time.sleep(0.01)\n",
    "        # env.render()\n",
    "        action = get_action(weights, observation) # 获取当前权值下的决策动作\n",
    "        observation, reward, terminated, truncated, info = env.step(action)# 执行动作并获取这一动作下的下一时间步长状态\n",
    "        sum_reward += reward\n",
    "        # print(sum_reward, action, observation, reward, done, info)\n",
    "        if terminated:# 如若游戏结束，返回\n",
    "            break\n",
    "    return sum_reward\n",
    "\n",
    "\n",
    "def get_weights_by_random_guess():\n",
    "# 选取随机猜测的5个随机权值\n",
    "    return np.random.rand(5)\n",
    "\n",
    "def get_weights_by_hill_climbing(best_weights):\n",
    "# 通过爬山算法选取权值（在当前最好权值上加入随机值）\n",
    "    return best_weights + np.random.normal(0, 0.1, 5)\n",
    "\n",
    "def get_best_result(algo=\"random_guess\"):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    np.random.seed(10)\n",
    "    best_reward = 0 # 初始最佳奖励\n",
    "    best_weights = np.random.rand(5) # 初始权值为随机取值\n",
    "\n",
    "    for iter in range(10000):# 迭代10000次\n",
    "        cur_weights = None\n",
    "\n",
    "        if algo == \"hill_climbing\": # 选取动作决策的算法 \n",
    "            # print(best_weights)\n",
    "            cur_weights = get_weights_by_hill_climbing(best_weights)\n",
    "        else: # 若为随机猜测算法，则选取随机权值\n",
    "            cur_weights = get_weights_by_random_guess()\n",
    "        # 获取当前权值的模型控制的奖励和\n",
    "        cur_sum_reward = get_sum_reward_by_weights(env, cur_weights)\n",
    "\n",
    "        # print(cur_sum_reward, cur_weights)\n",
    "        # 更新当前最优权值\n",
    "        if cur_sum_reward > best_reward:\n",
    "            best_reward = cur_sum_reward\n",
    "            best_weights = cur_weights\n",
    "        # 达到最佳奖励阈值后结束\n",
    "        if best_reward >= 200:\n",
    "            break\n",
    "\n",
    "    print(iter, best_reward, best_weights)\n",
    "    return best_reward, best_weights\n",
    "\n",
    "# 程序从这里开始执行\n",
    "best_result = get_best_result(\"hill_climbing\")\n",
    "print(best_result) # 调用爬山算法寻优并输出结果 \n",
    "best_parameters_1 = best_result[1][0:-1]\n",
    "\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "# get_sum_reward_by_weights(env, [0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b957cf",
   "metadata": {},
   "source": [
    "## 以下算法是Chat GPT提供的，它的特点是：\n",
    "1. 算法清晰明了，容易理解；\n",
    "2. 门槛较高，性能较好，但是容易不收敛，就是循环结束也拿不到符合条件的参数；\n",
    "3. 他用了10个episode的平均值作为评价标准，进一步提高了算法的门槛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dd99bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward when Train Ends: 392.4\n",
      "Total reward: 504.0\n",
      "Best Parameters: [-0.23072639  0.52300992 -0.35949529  1.09515392]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def run_episode(env, parameters):\n",
    "    observation = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    for _ in range(1000):\n",
    "        action = 0 if np.matmul(parameters, observation) < 0 else 1\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def train():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    best_params = None\n",
    "    best_reward = 0\n",
    "    noise_scaling = 0.1\n",
    "    episodes_per_update = 10\n",
    "\n",
    "    for _ in range(10000):\n",
    "        parameters = best_params if best_params is not None else np.random.rand(4) * 2 - 1\n",
    "        parameters += (np.random.rand(4) * 2 - 1) * noise_scaling\n",
    "        reward_sum = 0\n",
    "        for _ in range(episodes_per_update):\n",
    "            reward_sum += run_episode(env, parameters)\n",
    "        \n",
    "        average_reward = reward_sum / episodes_per_update\n",
    "        if average_reward > best_reward:\n",
    "            best_reward = average_reward\n",
    "            best_params = parameters\n",
    "            if average_reward >= 350.0:\n",
    "                print(f\"Average Reward when Train Ends: {average_reward}\")\n",
    "                break\n",
    "\n",
    "    return best_params\n",
    "\n",
    "best_parameters_2 = train()\n",
    "\n",
    "#best_parameters = [ 0.70975396, -0.30061841,  0.72104549,  1.16557572]\n",
    "#best_parameters = [-0.40743724,  0.51189324,  3.65836102,  0.52175062]\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "total_reward = run_episode(env, best_parameters_2)\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"Best Parameters: {best_parameters_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c6464",
   "metadata": {},
   "source": [
    "## 以下函数对CartPole问题解进行测试与评价，取多次测试奖赏值的平均分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e8110af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameters(best_params, num_episodes):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    sum_score = 0\n",
    "    for episode in range(num_episodes):\n",
    "        observation = env.reset()[0]\n",
    "        total_reward = 0\n",
    "\n",
    "        # 每次测试的最大步数\n",
    "        max_steps = 1000\n",
    "        for step in range(max_steps):\n",
    "            #env.render()\n",
    "            action = 0 if np.matmul(best_params, observation) < 0 else 1\n",
    "            observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if terminated:\n",
    "                #print(f\"Episode {episode + 1}: {total_reward} steps\")\n",
    "                break\n",
    "        sum_score += total_reward\n",
    "    \n",
    "    print(f\"Average Score: {sum_score/num_episodes}\")\n",
    "    env.close()\n",
    "    \n",
    "    #return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4767c7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 140.22\n"
     ]
    }
   ],
   "source": [
    "test_parameters(best_parameters_1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6df7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
